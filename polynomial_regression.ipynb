{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a02979-3391-4927-a5d2-2d5e7f5cd399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from astropy.table import Table\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_scatter_density\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "from plot_utils import plot_kiel_scatter_density"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d354ea32",
   "metadata": {},
   "source": [
    "### Read in the cleaned APOGEE data and make a Kiel Diagram for the whole sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc6082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "apogee_path = Path(\"./data/apogee_cleaned.parquet\")\n",
    "apogee_cat = pd.read_parquet(apogee_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177dad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_kiel, ax_kiel = plot_kiel_scatter_density(\n",
    "    apogee_cat['TEFF'],\n",
    "    apogee_cat['LOGG'],\n",
    "    apogee_cat['FE_H']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b162ca2",
   "metadata": {},
   "source": [
    "### Train-test split and fit a simple linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c3954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple linear model to predict feh from teff and logg\n",
    "# perform train test split and normalize the data\n",
    "\n",
    "\n",
    "X = apogee_cat[['TEFF', 'LOGG']]\n",
    "y = apogee_cat['FE_H']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=42)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#Print the sizes of the training and testing sets\n",
    "print(f\"Training set size: {X_train_scaled.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test_scaled.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ffabcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "# Predict FE_H values\n",
    "predicted_feh = model.predict(X_test_scaled)\n",
    "\n",
    "# Plot the kiel diagram for the predicted values (test set only)\n",
    "fig_kiel, ax_kiel = plot_kiel_scatter_density(\n",
    "    X_test['TEFF'],\n",
    "    X_test['LOGG'], \n",
    "    predicted_feh,\n",
    "    title='Kiel Diagram - Linear model',\n",
    "    colorbar_label='Predicted [Fe/H]',\n",
    ")\n",
    "\n",
    "#Print the slope and intercept of the linear model\n",
    "print(f\"Slope: {model.coef_}, Intercept: {model.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e588129e",
   "metadata": {},
   "source": [
    "### Polynomial Regression (with Linear Algebra Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348b04c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def polynomial_regression_lin_alg(X_train_scaled, X_test_scaled, y_train, test_teff, test_logg, degree):\n",
    "    \"\"\"\n",
    "    Fits a polynomial regression model and plots the Kiel diagram with predicted values.\n",
    "    \n",
    "    Args:\n",
    "        X_train_scaled: Scaled training features\n",
    "        X_test_scaled: Scaled test features  \n",
    "        y_train: Training target values\n",
    "        test_teff: Test set effective temperatures\n",
    "        test_logg: Test set surface gravities\n",
    "        degree: Degree of polynomial features\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (poly_model, predicted_feh_poly, fig, ax, density_map)\n",
    "    \"\"\"\n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=True)\n",
    "    # Transform the training and testing data\n",
    "    X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "    X_test_poly = poly.transform(X_test_scaled)\n",
    "    \n",
    "    # Fit the polynomial regression model\n",
    "    poly_model = LinearRegression(fit_intercept=False)\n",
    "    poly_model.fit(X_train_poly, y_train)\n",
    "    \n",
    "    # Predict FE_H values using the polynomial model\n",
    "    predicted_feh_poly = poly_model.predict(X_test_poly)\n",
    "\n",
    "    return poly_model, predicted_feh_poly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d493ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate polynomial regression plots for different degrees\n",
    "polynomial_degrees = [2, 3]\n",
    "models = {}\n",
    "\n",
    "for degree in polynomial_degrees:\n",
    "    print(f\"\\nGenerating polynomial regression of degree {degree}...\")\n",
    "    model, predictions = polynomial_regression_lin_alg(\n",
    "        X_train_scaled, X_test_scaled, y_train, X_test[\"TEFF\"], X_test[\"LOGG\"], degree\n",
    "    )\n",
    "    # Plot the kiel diagram for the predicted values (test set only)\n",
    "    fig, ax = plot_kiel_scatter_density(\n",
    "        X_test[\"TEFF\"],\n",
    "        X_test[\"LOGG\"], \n",
    "        predictions,\n",
    "        title=f'Polynomial regression of degree {degree} | Number of parameters: {len(model.coef_)}',\n",
    "        colorbar_label='Predicted [Fe/H]',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36609dd1",
   "metadata": {},
   "source": [
    "### Polynomial Regression with Gradient Descent Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d18115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionGD(pl.LightningModule):\n",
    "    def __init__(self, input_dim, output_dim=1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() # Saves degree, input_dim, learning_rate\n",
    "\n",
    "        self.linear_layer = nn.Linear(input_dim, output_dim, bias=False) # Output is 1 (e.g., FE_H)\n",
    "    def forward(self, x_batch_tensor):\n",
    "        return self.linear_layer(x_batch_tensor)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.functional.mse_loss(y_hat, y)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.functional.mse_loss(y_hat, y)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.functional.mse_loss(y_hat, y)\n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.SGD(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2b8fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_regression_gradient_descent(X_train_scaled, X_test_scaled, y_train, test_teff, test_logg, degree, **kwargs):\n",
    "       \n",
    "    # Hyperparameters from kwargs with defaults\n",
    "    learning_rate = kwargs.get('learning_rate', 1e-3)\n",
    "    max_epochs = kwargs.get('max_epochs', 1000)\n",
    "    batch_size = kwargs.get('batch_size', -1)  # -1 means use full batch\n",
    "    patience = kwargs.get('patience', 10)\n",
    "    val_split_ratio = kwargs.get('val_split_ratio', 0.2)\n",
    "    num_workers = kwargs.get('num_workers', 0)\n",
    "    accelerator = kwargs.get('accelerator', 'auto')\n",
    "    devices = kwargs.get('devices', 'auto')\n",
    "    checkpoint_dir = kwargs.get('checkpoint_dir', f'poly_regression_pt_checkpoints/deg_{degree}/')\n",
    "\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    if batch_size <= 0:\n",
    "        batch_size = len(X_train_scaled)\n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=True)\n",
    "    # Transform the training and testing data\n",
    "    X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "    X_test_poly = poly.transform(X_test_scaled)\n",
    "\n",
    "    # Prepare data: Convert numpy arrays and pandas Series to PyTorch Tensors\n",
    "    X_train_tensor = torch.tensor(X_train_poly, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test_poly, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test.values.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "    # Create full training dataset\n",
    "    full_train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "    # Split full training dataset into train and validation sets\n",
    "    num_train_samples = len(full_train_dataset)\n",
    "    num_val_samples = int(val_split_ratio * num_train_samples)\n",
    "    num_actual_train_samples = num_train_samples - num_val_samples\n",
    "\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        full_train_dataset, [num_actual_train_samples, num_val_samples],\n",
    "        generator=torch.Generator().manual_seed(42) # for reproducibility\n",
    "    )\n",
    "\n",
    "    persistent_workers_flag = num_workers > 0\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, persistent_workers=persistent_workers_flag)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, persistent_workers=persistent_workers_flag)\n",
    "    test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=batch_size, num_workers=num_workers, persistent_workers=persistent_workers_flag)\n",
    "    # Callbacks\n",
    "    early_stop_callback = EarlyStopping(monitor='val_loss', patience=patience, verbose=False, mode='min')\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        dirpath=checkpoint_dir,\n",
    "        filename=f'poly_deg{degree}-{{epoch:02d}}-{{val_loss:.4f}}',\n",
    "        save_top_k=1,\n",
    "        mode='min',\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = LinearRegressionGD(\n",
    "        input_dim=X_train_poly.shape[1],  # Number of polynomial features\n",
    "        output_dim=1,  # Predicting a single value (e.g., FE_H)\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        callbacks=[early_stop_callback, checkpoint_callback],\n",
    "        accelerator=accelerator,\n",
    "        devices=devices,\n",
    "        logger=False, # Uses TensorBoardLogger by default should move to WandB.\n",
    "        enable_progress_bar=True,\n",
    "        deterministic=True # For reproducibility\n",
    "    )\n",
    "    pl.seed_everything(42, workers=True) # For reproducibility\n",
    "\n",
    "    # Train the model\n",
    "    print(f\"\\nTraining PyTorch Polynomial Regression (degree {degree})...\")\n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "    # Load the best model from checkpoint\n",
    "    if checkpoint_callback.best_model_path:\n",
    "        print(f\"Loading best model from: {checkpoint_callback.best_model_path}\")\n",
    "        best_model = LinearRegressionGD.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "        best_model.eval() # Set to evaluation mode\n",
    "    else:\n",
    "        print(\"No best model path found from checkpoint callback. Using the model instance after fit.\")\n",
    "        best_model = model # Fallback, though best_model_path should ideally exist\n",
    "        best_model.eval()\n",
    "\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    predicted_feh_test = torch.concat(trainer.predict(best_model, dataloaders=DataLoader(X_test_tensor, batch_size=batch_size)))\n",
    "    predicted_feh_train = torch.concat(trainer.predict(best_model, dataloaders=DataLoader(X_train_tensor, batch_size=batch_size)))\n",
    "\n",
    "    train_loss = nn.functional.mse_loss(predicted_feh_train, y_train_tensor).item()\n",
    "    test_loss = nn.functional.mse_loss(predicted_feh_test, y_test_tensor).item()\n",
    "\n",
    "    return best_model, np.squeeze(predicted_feh_test.numpy()), train_loss, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de90d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, predicted_feh,  train_loss, test_loss = polynomial_regression_gradient_descent(\n",
    "        X_train_scaled, X_test_scaled, y_train, X_test[\"TEFF\"], X_test[\"LOGG\"], degree=1,\n",
    "        learning_rate=1e-2,\n",
    "        max_epochs=10\n",
    "    )\n",
    "# print the training and test loss\n",
    "print(f\"Training loss: {train_loss:.4f}, Test loss: {test_loss:.4f}\")\n",
    "total_model_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total model parameters: {total_model_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5de3b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Kiel diagram\n",
    "fig, ax  = plot_kiel_scatter_density(\n",
    "    X_test[\"TEFF\"],\n",
    "    X_test[\"LOGG\"], \n",
    "    np.squeeze(predicted_feh.numpy()),\n",
    "    title=f'Polynomial regression of degree {degree} | Number of parameters: {total_model_params}',\n",
    "    colorbar_label='Predicted [Fe/H]',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9fe970",
   "metadata": {},
   "source": [
    "# TO BE Moved to a separate notebook on DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357f0f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the MLP model using PyTorch Lightning\n",
    "class MLP(pl.LightningModule):\n",
    "    def __init__(self, network_shape, input_dim, learning_rate=1e-3, output_dim=1):\n",
    "        super().__init__()\n",
    "        # Save hyperparameters (network_shape, input_dim, learning_rate, output_dim)\n",
    "        # This allows loading from checkpoint without re-passing these args\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        layers = []\n",
    "        current_dim = self.hparams.input_dim\n",
    "        for hidden_dim in self.hparams.network_shape:\n",
    "            layers.append(nn.Linear(current_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            current_dim = hidden_dim\n",
    "        layers.append(nn.Linear(current_dim, self.hparams.output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.functional.mse_loss(y_hat, y)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.functional.mse_loss(y_hat, y)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.functional.mse_loss(y_hat, y)\n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "# Function to fit MLP, make predictions, and plot Kiel diagram\n",
    "def fit_and_plot_mlp_regression(\n",
    "    X_train_scaled_np, X_test_scaled_np, y_train_series, y_test_series,\n",
    "    test_teff_series, test_logg_series, network_shape, **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Fits an MLP regression model using PyTorch Lightning, plots the Kiel diagram with predicted values.\n",
    "\n",
    "    Args:\n",
    "        X_train_scaled_np (np.ndarray): Scaled training features.\n",
    "        X_test_scaled_np (np.ndarray): Scaled test features.\n",
    "        y_train_series (pd.Series): Training target values.\n",
    "        y_test_series (pd.Series): Test target values.\n",
    "        test_teff_series (pd.Series): Test set effective temperatures.\n",
    "        test_logg_series (pd.Series): Test set surface gravities.\n",
    "        network_shape (list of int): List defining the number of neurons in each hidden layer.\n",
    "        **kwargs: Additional hyperparameters for training.\n",
    "            learning_rate (float): Optimizer learning rate. Default: 1e-3.\n",
    "            batch_size (int): Batch size for DataLoaders. Default: 256.\n",
    "            max_epochs (int): Maximum number of training epochs. Default: 100.\n",
    "            patience (int): Patience for EarlyStopping. Default: 10.\n",
    "            val_split_ratio (float): Fraction of training data to use for validation. Default: 0.2.\n",
    "            num_workers (int): Number of workers for DataLoader. Default: 0.\n",
    "            accelerator (str): PyTorch Lightning accelerator ('cpu', 'gpu', 'auto'). Default: 'auto'.\n",
    "            devices (any): PyTorch Lightning devices. Default: 'auto'.\n",
    "            checkpoint_dir (str): Directory to save model checkpoints. Default: 'mlp_checkpoints/'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (best_model, predicted_feh_mlp, fig, ax, density_map)\n",
    "            best_model (MLP): The trained MLP model (best checkpoint).\n",
    "            predicted_feh_mlp (np.ndarray): Predicted [Fe/H] values on the test set.\n",
    "            fig (matplotlib.figure.Figure): The figure object for the Kiel diagram.\n",
    "            ax (matplotlib.axes.Axes): The axes object for the Kiel diagram.\n",
    "            density_map (matplotlib.collections.PathCollection): The scatter density plot object.\n",
    "    \"\"\"\n",
    "    # Hyperparameters from kwargs with defaults\n",
    "    learning_rate = kwargs.get('learning_rate', 1e-3)\n",
    "    batch_size = kwargs.get('batch_size', 256)\n",
    "    max_epochs = kwargs.get('max_epochs', 100)\n",
    "    patience = kwargs.get('patience', 10)\n",
    "    val_split_ratio = kwargs.get('val_split_ratio', 0.2)\n",
    "    num_workers = kwargs.get('num_workers', 0) # Default to 0 for broader compatibility\n",
    "    accelerator = kwargs.get('accelerator', 'auto')\n",
    "    devices = kwargs.get('devices', 'auto')\n",
    "    checkpoint_dir = kwargs.get('checkpoint_dir', 'mlp_checkpoints/')\n",
    "    \n",
    "    # Ensure checkpoint directory exists\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare data: Convert numpy arrays and pandas Series to PyTorch Tensors\n",
    "    X_train_tensor = torch.tensor(X_train_scaled_np, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train_series.values.reshape(-1, 1), dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test_scaled_np, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test_series.values.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "    # Create full training dataset\n",
    "    full_train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "    # Split full training dataset into actual train and validation sets\n",
    "    num_train_samples = len(full_train_dataset)\n",
    "    num_val_samples = int(val_split_ratio * num_train_samples)\n",
    "    num_actual_train_samples = num_train_samples - num_val_samples\n",
    "\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        full_train_dataset, [num_actual_train_samples, num_val_samples],\n",
    "        generator=torch.Generator().manual_seed(42) # for reproducibility\n",
    "    )\n",
    "    \n",
    "    persistent_workers_flag = num_workers > 0\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, persistent_workers=persistent_workers_flag)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, persistent_workers=persistent_workers_flag)\n",
    "    \n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, persistent_workers=persistent_workers_flag)\n",
    "\n",
    "    # Callbacks\n",
    "    early_stop_callback = EarlyStopping(monitor='val_loss', patience=patience, verbose=False, mode='min')\n",
    "    # Sanitize network_shape for filename\n",
    "    shape_str = \"_\".join(map(str, network_shape))\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        dirpath=checkpoint_dir,\n",
    "        filename=f'mlp-shape_{shape_str}-{{epoch:02d}}-{{val_loss:.4f}}',\n",
    "        save_top_k=1,\n",
    "        mode='min',\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = MLP(\n",
    "        network_shape=network_shape,\n",
    "        input_dim=X_train_scaled_np.shape[1],\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        callbacks=[early_stop_callback, checkpoint_callback],\n",
    "        accelerator=accelerator,\n",
    "        devices=devices,\n",
    "        logger=True, # Uses TensorBoardLogger by default, logs to lightning_logs/\n",
    "        enable_progress_bar=True,\n",
    "        deterministic=True # For reproducibility, might impact performance\n",
    "    )\n",
    "    pl.seed_everything(42, workers=True) # For reproducibility\n",
    "\n",
    "    # Train the model\n",
    "    print(f\"\\nTraining MLP with network shape: {network_shape}...\")\n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "    # Load the best model from checkpoint\n",
    "    print(f\"Loading best model from: {checkpoint_callback.best_model_path}\")\n",
    "    best_model = MLP.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "    best_model.eval() # Set to evaluation mode\n",
    "\n",
    "    # Evaluate on the test set (optional, but good practice)\n",
    "    test_results = trainer.test(best_model, dataloaders=test_loader, verbose=False)\n",
    "    print(f\"Test results for MLP shape {network_shape}: {test_results}\")\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    with torch.no_grad():\n",
    "        predicted_feh_mlp_tensor = best_model(X_test_tensor)\n",
    "    predicted_feh_mlp = predicted_feh_mlp_tensor.cpu().numpy().flatten()\n",
    "\n",
    "    # Plot the Kiel diagram using the existing plot_kiel_scatter_density function\n",
    "    fig, ax, density_map = plot_kiel_scatter_density(\n",
    "        test_teff_series,\n",
    "        test_logg_series,\n",
    "        predicted_feh_mlp,\n",
    "        title=f'Kiel Diagram - MLP Regression (Shape: {network_shape})',\n",
    "        colorbar_label='Predicted [Fe/H] (MLP)',\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    return best_model, predicted_feh_mlp, fig, ax, density_map\n",
    "\n",
    "\n",
    "# Example usage: Define network shapes and run the MLP regression function\n",
    "# These variables are assumed to be defined in previous cells:\n",
    "# X_train_scaled, X_test_scaled, y_train, y_test, test_teff, test_logg\n",
    "# plot_kiel_scatter_density (function)\n",
    "\n",
    "mlp_network_shapes = [\n",
    "    [64],             # Single hidden layer with 64 neurons\n",
    "    [64, 32],         # Two hidden layers: 64 neurons, then 32 neurons\n",
    "    [128, 64, 32]     # Three hidden layers\n",
    "]\n",
    "mlp_results = {}\n",
    "\n",
    "# Set common hyperparameters for all MLP runs, can be overridden in kwargs\n",
    "mlp_hyperparams = {\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 512,\n",
    "    'max_epochs': 50,  # Adjust as needed, can be short for demonstration\n",
    "    'patience': 5,     # Adjust as needed\n",
    "    'num_workers': 0,  # Set to 0 for simplicity, os.cpu_count() for performance if env supports\n",
    "    'accelerator': 'gpu', 'devices': 1, # Uncomment if GPU is available\n",
    "}\n",
    "\n",
    "for shape in mlp_network_shapes:\n",
    "    print(\"-\" * 50)\n",
    "    shape_key = f'shape_{\"_\".join(map(str, shape))}'\n",
    "    \n",
    "    model_mlp, predictions_mlp, fig_mlp, ax_mlp, dm_mlp = fit_and_plot_mlp_regression(\n",
    "        X_train_scaled, X_test_scaled, y_train, y_test,\n",
    "        test_teff, test_logg,\n",
    "        network_shape=shape,\n",
    "        **mlp_hyperparams \n",
    "    )\n",
    "    mlp_results[shape_key] = {\n",
    "        'model': model_mlp,\n",
    "        'predictions': predictions_mlp,\n",
    "        'fig': fig_mlp,\n",
    "        'ax': ax_mlp,\n",
    "        'density_map': dm_mlp\n",
    "    }\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"MLP regression experiments complete.\")\n",
    "print(f\"Results stored in mlp_results dictionary with keys: {list(mlp_results.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa631b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40347f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage for PyTorch Polynomial Regression\n",
    "# Assumes X_train_scaled, X_test_scaled, y_train, y_test, test_teff, test_logg are defined\n",
    "\n",
    "poly_pt_degrees = [2, 3, 5]  # Degrees to test, 5 instead of 10 for potentially faster run\n",
    "poly_pt_results = {}\n",
    "\n",
    "# Common hyperparameters for PyTorch Polynomial Regression runs\n",
    "poly_pt_hyperparams = {\n",
    "    'learning_rate': 0.01,     # Polynomial regression might benefit from a slightly higher LR\n",
    "    'batch_size': 512,\n",
    "    'max_epochs': 100,         # Can be adjusted; set lower for quicker tests\n",
    "    'patience': 10,            # For early stopping\n",
    "    'num_workers': 0,          # Set to 0 for simplicity, can be os.cpu_count() for performance\n",
    "    'accelerator': 'gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    'devices': 1 if torch.cuda.is_available() else 'auto',\n",
    "    # checkpoint_dir is handled within fit_and_plot_poly_regression_pytorch\n",
    "}\n",
    "\n",
    "print(\"Starting PyTorch Polynomial Regression experiments...\")\n",
    "for degree_val in poly_pt_degrees:\n",
    "    print(\"-\" * 50)\n",
    "    degree_key = f'degree_{degree_val}'\n",
    "    \n",
    "    # Ensure all required variables are available in the notebook's global scope\n",
    "    # These should have been defined in cells above where X, y were split and scaled.\n",
    "    model_poly_pt, predictions_poly_pt, fig_poly_pt, ax_poly_pt, dm_poly_pt = fit_and_plot_poly_regression_pytorch(\n",
    "        X_train_scaled, X_test_scaled, y_train, y_test,\n",
    "        test_teff, test_logg, # These are pd.Series for Teff and Logg from the X_test dataframe\n",
    "        degree=degree_val,\n",
    "        **poly_pt_hyperparams \n",
    "    )\n",
    "    poly_pt_results[degree_key] = {\n",
    "        'model': model_poly_pt,\n",
    "        'predictions': predictions_poly_pt,\n",
    "        'fig': fig_poly_pt,\n",
    "        'ax': ax_poly_pt,\n",
    "        'density_map': dm_poly_pt\n",
    "    }\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"PyTorch Polynomial Regression experiments complete.\")\n",
    "print(f\"Results stored in poly_pt_results dictionary with keys: {list(poly_pt_results.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6bb88c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533ae38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
