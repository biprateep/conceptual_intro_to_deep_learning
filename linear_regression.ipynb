{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a02979-3391-4927-a5d2-2d5e7f5cd399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from astropy.table import Table\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import math\n",
    "from plot_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d354ea32",
   "metadata": {},
   "source": [
    "### Read in the cleaned APOGEE data and make a Kiel Diagram for the whole sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc6082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "apogee_path = Path(\"./data/apogee_cleaned.parquet\")\n",
    "apogee_cat = pd.read_parquet(apogee_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177dad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_kiel, ax_kiel = plot_kiel_scatter_density(\n",
    "    apogee_cat['TEFF'],\n",
    "    apogee_cat['LOGG'],\n",
    "    apogee_cat['FE_H']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b162ca2",
   "metadata": {},
   "source": [
    "### Train-test split and fit a simple linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd6f953-57dd-4edc-972a-c72d6178b8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_features(x, degree=1, add_bias=True):\n",
    "    if x.ndim == 1:\n",
    "        x = x[:, np.newaxis]\n",
    "    \n",
    "    n_samples, n_features = x.shape\n",
    "    features = []\n",
    "    for d in range(1, degree + 1):\n",
    "        features.append(np.sin(d * x))\n",
    "        features.append(np.cos(d * x))\n",
    "    if add_bias:\n",
    "        features.append(np.ones((n_samples, 1)))\n",
    "        \n",
    "    return np.concatenate(features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c3954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple linear model to predict feh from teff and logg\n",
    "# perform train test split and normalize the data\n",
    "\n",
    "\n",
    "X = apogee_cat[['TEFF', 'LOGG']]\n",
    "y = apogee_cat['FE_H']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.999, random_state=42)\n",
    "\n",
    "_, X_test, _, y_test = train_test_split(X_test, y_test, test_size=0.002, random_state=42)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train).astype(\"float64\")\n",
    "X_test_scaled = scaler.transform(X_test).astype(\"float64\")\n",
    "y_train = y_train.astype(\"float64\")\n",
    "y_test = y_test.astype(\"float64\")\n",
    "\n",
    "#Print the sizes of the training and testing sets\n",
    "print(f\"Training set size: {X_train_scaled.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test_scaled.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee2f64a-708e-4572-ac09-8fd5f106ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the kiel diagram for the predicted values (test set only)\n",
    "fig_kiel, ax_kiel = plot_kiel_scatter_density(\n",
    "    X_train['TEFF'],\n",
    "    X_train['LOGG'], \n",
    "    y_train,\n",
    "    title='Kiel Diagram - Train Set',\n",
    "    colorbar_label='Predicted [Fe/H]',\n",
    "    scatter = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b267be36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# n = 35\n",
    "# k=2\n",
    "# print(math.comb(n + k , k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ffabcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model = LinearRegression()\n",
    "# model = Ridge()\n",
    "# model.fit(X_train_scaled, y_train)\n",
    "# # Predict FE_H values\n",
    "# predicted_feh = model.predict(X_test_scaled)\n",
    "\n",
    "# # Plot the kiel diagram for the predicted values (test set only)\n",
    "# fig_kiel, ax_kiel = plot_kiel_scatter_density(\n",
    "#     X_test['TEFF'],\n",
    "#     X_test['LOGG'], \n",
    "#     predicted_feh,\n",
    "#     title='Kiel Diagram - Linear model',\n",
    "#     colorbar_label='Predicted [Fe/H]',\n",
    "# )\n",
    "\n",
    "# #Print the slope and intercept of the linear model\n",
    "# print(f\"Slope: {model.coef_}, Intercept: {model.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e588129e",
   "metadata": {},
   "source": [
    "### Polynomial Regression (with Linear Algebra Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348b04c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def polynomial_regression_lin_alg(X_train_scaled, X_test_scaled, y_train, test_teff, test_logg, degree, use_fourier_features=False, use_pinv=False):\n",
    "    \"\"\"\n",
    "    Fits a polynomial regression model and plots the Kiel diagram with predicted values.\n",
    "    \n",
    "    Args:\n",
    "        X_train_scaled: Scaled training features\n",
    "        X_test_scaled: Scaled test features  \n",
    "        y_train: Training target values\n",
    "        test_teff: Test set effective temperatures\n",
    "        test_logg: Test set surface gravities\n",
    "        degree: Degree of polynomial features\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (poly_model, predicted_feh_poly, fig, ax, density_map)\n",
    "    \"\"\"\n",
    "    if use_fourier_features:\n",
    "        X_train_poly = fourier_features(X_train_scaled,degree=degree, add_bias=True)\n",
    "        X_test_poly = fourier_features(X_test_scaled,degree=degree, add_bias=True)\n",
    "    else:\n",
    "        # # Create polynomial features\n",
    "        poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "        # Transform the training and testing data\n",
    "        X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "        X_test_poly = poly.transform(X_test_scaled)\n",
    "\n",
    "\n",
    "    #Rescale after feature crafting\n",
    "    poly_scale = StandardScaler()\n",
    "    poly_scale.fit(X_train_poly)\n",
    "    X_train_poly_scaled = poly_scale.transform(X_train_poly)\n",
    "    X_test_poly_scaled = poly_scale.transform(X_test_poly)\n",
    "\n",
    "\n",
    "    if use_pinv:\n",
    "         # Alternate fitting algorithm\n",
    "        beta_hat = np.linalg.pinv(X_train_poly_scaled) @ y_train\n",
    "        predicted_feh_train = X_train_poly_scaled @ beta_hat\n",
    "        predicted_feh_test = X_test_poly_scaled @ beta_hat\n",
    "        poly_model = beta_hat\n",
    "    else:\n",
    "        # Fit the polynomial regression model\n",
    "        poly_model = LinearRegression(fit_intercept=True)\n",
    "        # poly_model = Ridge(alpha = 5 ,fit_intercept=True)\n",
    "        poly_model.fit(X_train_poly_scaled, y_train)\n",
    "        # Predict FE_H values using the polynomial model\n",
    "        predicted_feh_test = poly_model.predict(X_test_poly_scaled)\n",
    "        predicted_feh_train = poly_model.predict(X_train_poly_scaled)\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    train_loss = np.mean((predicted_feh_train-y_train)**2)\n",
    "    test_loss =  np.mean((predicted_feh_test-y_test)**2)\n",
    "\n",
    "\n",
    "    return poly_model, predicted_feh_test, train_loss, test_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d493ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1500\n",
    "\n",
    "# Generate polynomial regression plots for different degrees\n",
    "polynomial_degrees = [i+1 for i in range(n_features)]\n",
    "\n",
    "# select N random integers instead of sequential degrees\n",
    "# polynomial_degrees = np.random.choice(n_features, size=n_features, replace=False)\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "\n",
    "use_fourier_features = True\n",
    "\n",
    "pbar = tqdm(polynomial_degrees)\n",
    "for degree in pbar:\n",
    "    pbar.set_description(f\"Generating polynomial regression of degree {degree}...\")\n",
    "    model, predictions,train_loss, test_loss = polynomial_regression_lin_alg(\n",
    "        X_train_scaled, X_test_scaled, y_train, X_test[\"TEFF\"], X_test[\"LOGG\"], degree,\n",
    "        use_fourier_features=use_fourier_features,\n",
    "        use_pinv = True\n",
    "        \n",
    "    )\n",
    "    train_loss_list.append(train_loss)\n",
    "    test_loss_list.append(test_loss)\n",
    "    # Plot the kiel diagram for the predicted values (test set only)\n",
    "# fig, ax = plot_kiel_scatter_density(\n",
    "#     X_test[\"TEFF\"],\n",
    "#     X_test[\"LOGG\"], \n",
    "#     predictions,\n",
    "#     title=f'Polynomial regression of degree {degree} | Number of parameters: {len(model.coef_)}',\n",
    "#     colorbar_label='Predicted [Fe/H]',\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eddefe3-39cb-4710-befa-73ee0bbe04ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynomial_degrees = [i+1 for i in range(70)]\n",
    "if use_fourier_features:\n",
    "    num_features = [2*n+1 for n in polynomial_degrees]\n",
    "else:\n",
    "    num_features = [math.comb(n + 2 , 2) for n in polynomial_degrees]\n",
    "# \n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(5,3))\n",
    "ax.scatter(num_features, test_loss_list, label=\"Test Loss\")\n",
    "ax.scatter(num_features, train_loss_list,label=\"Train Loss\")\n",
    "ax.axvline(len(X_train), ls=\"--\", color=\"k\", label=\"Interpolation Threshold\")\n",
    "\n",
    "\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xscale(\"log\")\n",
    "# ax.set_ylim(ymax=1e5)\n",
    "ax.set_xlabel(\"Number of parameters\")\n",
    "ax.set_ylabel(\"Loss Value\")\n",
    "plt.legend(frameon=False,loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb0c64d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
