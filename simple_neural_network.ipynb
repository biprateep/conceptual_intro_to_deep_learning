{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a02979-3391-4927-a5d2-2d5e7f5cd399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from astropy.table import Table\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_scatter_density\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import math\n",
    "from plot_utils import plot_kiel_scatter_density"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d354ea32",
   "metadata": {},
   "source": [
    "### Read in the cleaned APOGEE data and make a Kiel Diagram for the whole sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc6082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "apogee_path = Path(\"./data/apogee_cleaned.parquet\")\n",
    "apogee_cat = pd.read_parquet(apogee_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177dad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_kiel, ax_kiel = plot_kiel_scatter_density(\n",
    "    apogee_cat['TEFF'],\n",
    "    apogee_cat['LOGG'],\n",
    "    apogee_cat['FE_H']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b162ca2",
   "metadata": {},
   "source": [
    "### Train-test split and fit a simple linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c3954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple linear model to predict feh from teff and logg\n",
    "# perform train test split and normalize the data\n",
    "\n",
    "\n",
    "X = apogee_cat[['TEFF', 'LOGG']]\n",
    "y = apogee_cat['FE_H']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.999, random_state=42)\n",
    "\n",
    "_, X_test, _, y_test = train_test_split(X_test, y_test, test_size=0.002, random_state=42)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train).astype(\"float64\")\n",
    "X_test_scaled = scaler.transform(X_test).astype(\"float64\")\n",
    "y_train = y_train.astype(\"float64\")\n",
    "y_test = y_test.astype(\"float64\")\n",
    "\n",
    "#Print the sizes of the training and testing sets\n",
    "print(f\"Training set size: {X_train_scaled.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test_scaled.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee2f64a-708e-4572-ac09-8fd5f106ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the kiel diagram for the predicted values (test set only)\n",
    "fig_kiel, ax_kiel = plot_kiel_scatter_density(\n",
    "    X_train['TEFF'],\n",
    "    X_train['LOGG'], \n",
    "    y_train,\n",
    "    title='Kiel Diagram - Train Set',\n",
    "    colorbar_label='Predicted [Fe/H]',\n",
    "    scatter = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357f0f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the MLP model using PyTorch Lightning\n",
    "class MLP(pl.LightningModule):\n",
    "    def __init__(self, network_shape, input_dim, learning_rate=1e-3, output_dim=1):\n",
    "        super().__init__()\n",
    "        # Save hyperparameters (network_shape, input_dim, learning_rate, output_dim)\n",
    "        # This allows loading from checkpoint without re-passing these args\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        layers = []\n",
    "        current_dim = self.hparams.input_dim\n",
    "        for hidden_dim in self.hparams.network_shape:\n",
    "            layers.append(nn.Linear(current_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            current_dim = hidden_dim\n",
    "        layers.append(nn.Linear(current_dim, self.hparams.output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.functional.mse_loss(y_hat, y)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.functional.mse_loss(y_hat, y)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.functional.mse_loss(y_hat, y)\n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "# Function to fit MLP, make predictions, and plot Kiel diagram\n",
    "def fit_mlp_regression(\n",
    "    X_train_scaled_np, X_test_scaled_np, y_train_series, y_test_series,\n",
    "    test_teff_series, test_logg_series, network_shape, **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Fits an MLP regression model using PyTorch Lightning, plots the Kiel diagram with predicted values.\n",
    "\n",
    "    Args:\n",
    "        X_train_scaled_np (np.ndarray): Scaled training features.\n",
    "        X_test_scaled_np (np.ndarray): Scaled test features.\n",
    "        y_train_series (pd.Series): Training target values.\n",
    "        y_test_series (pd.Series): Test target values.\n",
    "        test_teff_series (pd.Series): Test set effective temperatures.\n",
    "        test_logg_series (pd.Series): Test set surface gravities.\n",
    "        network_shape (list of int): List defining the number of neurons in each hidden layer.\n",
    "        **kwargs: Additional hyperparameters for training.\n",
    "            learning_rate (float): Optimizer learning rate. Default: 1e-3.\n",
    "            batch_size (int): Batch size for DataLoaders. Default: 256.\n",
    "            max_epochs (int): Maximum number of training epochs. Default: 100.\n",
    "            patience (int): Patience for EarlyStopping. Default: 10.\n",
    "            val_split_ratio (float): Fraction of training data to use for validation. Default: 0.2.\n",
    "            num_workers (int): Number of workers for DataLoader. Default: 0.\n",
    "            accelerator (str): PyTorch Lightning accelerator ('cpu', 'gpu', 'auto'). Default: 'auto'.\n",
    "            devices (any): PyTorch Lightning devices. Default: 'auto'.\n",
    "            checkpoint_dir (str): Directory to save model checkpoints. Default: 'mlp_checkpoints/'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (best_model, predicted_feh_mlp, fig, ax, density_map)\n",
    "            best_model (MLP): The trained MLP model (best checkpoint).\n",
    "            predicted_feh_mlp (np.ndarray): Predicted [Fe/H] values on the test set.\n",
    "            fig (matplotlib.figure.Figure): The figure object for the Kiel diagram.\n",
    "            ax (matplotlib.axes.Axes): The axes object for the Kiel diagram.\n",
    "            density_map (matplotlib.collections.PathCollection): The scatter density plot object.\n",
    "    \"\"\"\n",
    "    # Hyperparameters from kwargs with defaults\n",
    "    learning_rate = kwargs.get('learning_rate', 1e-3)\n",
    "    batch_size = kwargs.get('batch_size', 256)\n",
    "    max_epochs = kwargs.get('max_epochs', 100)\n",
    "    patience = kwargs.get('patience', 10)\n",
    "    val_split_ratio = kwargs.get('val_split_ratio', 0.2)\n",
    "    num_workers = kwargs.get('num_workers', 0) # Default to 0 for broader compatibility\n",
    "    accelerator = kwargs.get('accelerator', 'auto')\n",
    "    devices = kwargs.get('devices', 'auto')\n",
    "    checkpoint_dir = kwargs.get('checkpoint_dir', 'mlp_checkpoints/')\n",
    "    \n",
    "    # Ensure checkpoint directory exists\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare data: Convert numpy arrays and pandas Series to PyTorch Tensors\n",
    "    X_train_tensor = torch.tensor(X_train_scaled_np, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train_series.values.reshape(-1, 1), dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test_scaled_np, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test_series.values.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "    # Create full training dataset\n",
    "    full_train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "    # Split full training dataset into actual train and validation sets\n",
    "    num_train_samples = len(full_train_dataset)\n",
    "    num_val_samples = int(val_split_ratio * num_train_samples)\n",
    "    num_actual_train_samples = num_train_samples - num_val_samples\n",
    "\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        full_train_dataset, [num_actual_train_samples, num_val_samples],\n",
    "        generator=torch.Generator().manual_seed(42) # for reproducibility\n",
    "    )\n",
    "    \n",
    "    persistent_workers_flag = num_workers > 0\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, persistent_workers=persistent_workers_flag)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, persistent_workers=persistent_workers_flag)\n",
    "    \n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, persistent_workers=persistent_workers_flag)\n",
    "\n",
    "    # Callbacks\n",
    "    early_stop_callback = EarlyStopping(monitor='val_loss', patience=patience, verbose=False, mode='min')\n",
    "    # Sanitize network_shape for filename\n",
    "    shape_str = \"_\".join(map(str, network_shape))\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        dirpath=checkpoint_dir,\n",
    "        filename=f'mlp-shape_{shape_str}-{{epoch:02d}}-{{val_loss:.4f}}',\n",
    "        save_top_k=1,\n",
    "        mode='min',\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = MLP(\n",
    "        network_shape=network_shape,\n",
    "        input_dim=X_train_scaled_np.shape[1],\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        callbacks=[early_stop_callback, checkpoint_callback],\n",
    "        accelerator=accelerator,\n",
    "        devices=devices,\n",
    "        logger=True, # Uses TensorBoardLogger by default, logs to lightning_logs/\n",
    "        enable_progress_bar=True,\n",
    "        deterministic=True # For reproducibility, might impact performance\n",
    "    )\n",
    "    pl.seed_everything(42, workers=True) # For reproducibility\n",
    "\n",
    "    # Train the model\n",
    "    print(f\"\\nTraining MLP with network shape: {network_shape}...\")\n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "    # Load the best model from checkpoint\n",
    "    print(f\"Loading best model from: {checkpoint_callback.best_model_path}\")\n",
    "    best_model = MLP.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "    best_model.eval() # Set to evaluation mode\n",
    "\n",
    "    # Evaluate on the test set (optional, but good practice)\n",
    "    test_results = trainer.test(best_model, dataloaders=test_loader, verbose=False)\n",
    "    print(f\"Test results for MLP shape {network_shape}: {test_results}\")\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    with torch.no_grad():\n",
    "        predicted_feh_mlp_tensor = best_model(X_test_tensor)\n",
    "    predicted_feh_mlp = predicted_feh_mlp_tensor.cpu().numpy().flatten()\n",
    "\n",
    "    # Plot the Kiel diagram using the existing plot_kiel_scatter_density function\n",
    "    \n",
    "\n",
    "    return best_model, predicted_feh_mlp\n",
    "\n",
    "\n",
    "# Example usage: Define network shapes and run the MLP regression function\n",
    "# These variables are assumed to be defined in previous cells:\n",
    "# X_train_scaled, X_test_scaled, y_train, y_test, test_teff, test_logg\n",
    "# plot_kiel_scatter_density (function)\n",
    "\n",
    "mlp_network_shapes = [\n",
    "    [64],             # Single hidden layer with 64 neurons\n",
    "    [64, 32],         # Two hidden layers: 64 neurons, then 32 neurons\n",
    "    [128, 64, 32]     # Three hidden layers\n",
    "]\n",
    "mlp_results = {}\n",
    "\n",
    "# Set common hyperparameters for all MLP runs, can be overridden in kwargs\n",
    "mlp_hyperparams = {\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 512,\n",
    "    'max_epochs': 50,  # Adjust as needed, can be short for demonstration\n",
    "    'patience': 5,     # Adjust as needed\n",
    "    'num_workers': 0,  # Set to 0 for simplicity, os.cpu_count() for performance if env supports\n",
    "    'accelerator': 'gpu', 'devices': 1, # Uncomment if GPU is available\n",
    "}\n",
    "\n",
    "for shape in mlp_network_shapes:\n",
    "    print(\"-\" * 50)\n",
    "    shape_key = f'shape_{\"_\".join(map(str, shape))}'\n",
    "\n",
    "    model_mlp, predictions_mlp = fit_and_plot_mlp_regression(\n",
    "        X_train_scaled, X_test_scaled, y_train, y_test,\n",
    "        test_teff, test_logg,\n",
    "        network_shape=shape,\n",
    "        **mlp_hyperparams \n",
    "    )\n",
    "    mlp_results[shape_key] = {\n",
    "        'model': model_mlp,\n",
    "        'predictions': predictions_mlp\n",
    "    }\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"MLP regression experiments complete.\")\n",
    "print(f\"Results stored in mlp_results dictionary with keys: {list(mlp_results.keys())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
